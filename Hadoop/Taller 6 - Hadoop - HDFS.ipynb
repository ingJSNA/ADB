{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 6 - Hadoop - HDFS\n",
    "\n",
    "Juan Navarro, <jsnavarroa@unal.edu.co>\n",
    "\n",
    "\n",
    "# Instalación\n",
    "\n",
    "```bash\n",
    "cd \"${HOME}/worskpace/BDA\"\n",
    "\n",
    "git clone https://github.com/jsnavarroa/docker-hadoop.git\n",
    "cd docker-hadoop\n",
    "\n",
    "# Install docker-compose\n",
    "conda create --name py3 python=3.6\n",
    "conda activate py3\n",
    "conda install -c conda-forge docker-compose\n",
    "\n",
    "# Run local\n",
    "docker-compose -f docker-compose-local.yml build\n",
    "docker-compose -f docker-compose-local.yml up -d\n",
    "\n",
    "```\n",
    "\n",
    "* Hadoop URLs:\n",
    "  * NameNode http://localhost:9870/dfshealth.html#tab-overview.\n",
    "  * HDFS hdfs://localhost:9800.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Enviroment variables\n",
    "#export JAVA_HOME=$(readlink -f /usr/bin/java | sed \"s:bin/java::\")\n",
    "\n",
    "echo \"JAVA_HOME=$JAVA_HOME\"\n",
    "\n",
    "mkdir -p \"./data/taller6_out/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica HDFS\n",
    "\n",
    "Después de instalado el cluster de Hadoop se ejecutan los siguientes comandos para la práctica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /casa\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-30 10:53 /casa/piso1\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-30 10:54 /casa/piso1/cocina\n",
      "-rw-r--r--   3 juan supergroup       5194 2018-11-30 10:54 /casa/piso1/cocina/estufa.jpg\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-30 10:54 /casa/piso1/sala\n",
      "-rw-r--r--   3 juan supergroup       3106 2018-11-30 10:54 /casa/piso1/sala/mesa.jpg\n",
      "-rw-r--r--   3 juan supergroup       5688 2018-11-30 10:54 /casa/piso1/sala/televisor.jpg\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-30 10:54 /casa/piso2\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-30 10:54 /casa/piso2/alcoba\n",
      "-rw-r--r--   3 juan supergroup       5859 2018-11-30 10:54 /casa/piso2/alcoba/cama.jpg\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-30 10:54 /casa/piso2/estudio\n",
      "-rw-r--r--   3 juan supergroup       9216 2018-11-30 10:54 /casa/piso2/estudio/libro.doc\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "\n",
    "# Cleaning\n",
    "$HADOOP_HOME/bin/hdfs dfs -rm -r /casa\n",
    "\n",
    "# Create house\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p /casa/piso1/sala\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p /casa/piso1/cocina\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p /casa/piso2/alcoba\n",
    "\n",
    "# Add furniture\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/mesa.jpg /casa/piso1/sala\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/estufa.jpg /casa/piso1/cocina\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/libro.doc /casa/piso2/alcoba\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/televisor.jpg /casa/piso2/alcoba\n",
    "\n",
    "# Renovation\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p /casa/piso2/estudio\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/cama.jpg /casa/piso2/alcoba\n",
    "$HADOOP_HOME/bin/hdfs dfs -mv /casa/piso2/alcoba/televisor.jpg /casa/piso1/sala/televisor.jpg\n",
    "$HADOOP_HOME/bin/hdfs dfs -mv /casa/piso2/alcoba/libro.doc /casa/piso2/estudio/libro.doc\n",
    "\n",
    "# Check\n",
    "$HADOOP_HOME/bin/hdfs dfs -ls -R /casa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio de conteo de términos\n",
    "\n",
    "## I. Computar el valor de pi en paralelo en 5 nodos con 5 \"samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Maps  = 5\n",
      "Samples per Map = 5\n",
      "Wrote input for Map #0\n",
      "Wrote input for Map #1\n",
      "Wrote input for Map #2\n",
      "Wrote input for Map #3\n",
      "Wrote input for Map #4\n",
      "Starting Job\n",
      "Job Finished in 1.562 seconds\n",
      "Estimated value of Pi is 3.68000000000000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-30 10:54:16,884 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-11-30 10:54:16,926 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:270)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:147)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "HADOOP_EXAMPLES_JAR=\"${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar\"\n",
    "\n",
    "$HADOOP_HOME/bin/hadoop --loglevel WARN jar $HADOOP_EXAMPLES_JAR pi 5 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Frecuencia de Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\n",
      "-rw-r--r--   3 juan supergroup     111448 2018-11-30 10:54 biblia/input/xaa.txt\n",
      "-rw-r--r--   3 juan supergroup     127578 2018-11-30 10:54 biblia/input/xab.txt\n",
      "-rw-r--r--   3 juan supergroup     151478 2018-11-30 10:54 biblia/input/xac.txt\n",
      "-rw-r--r--   3 juan supergroup     190377 2018-11-30 10:54 biblia/input/xad.txt\n",
      "-rw-r--r--   3 juan supergroup     167416 2018-11-30 10:54 biblia/input/xae.txt\n",
      "-rw-r--r--   3 juan supergroup     131772 2018-11-30 10:54 biblia/input/xaf.txt\n",
      "-rw-r--r--   3 juan supergroup     121707 2018-11-30 10:54 biblia/input/xag.txt\n",
      "Deleted biblia/output\n",
      "áspides\t1\n",
      "átate\t1\n",
      "échala\t1\n",
      "échalo\t3\n",
      "échate\t4\n",
      "él\t592\n",
      "él!\t1\n",
      "él,\t215\n",
      "él.\t131\n",
      "él:\t8\n",
      "él;\t43\n",
      "él?\t8\n",
      "éramos\t5\n",
      "ése\t16\n",
      "ése,\t1\n",
      "ésta\t6\n",
      "ésta,\t4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-30 10:54:26,912 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-11-30 10:54:26,953 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:270)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:147)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2018-11-30 10:54:26,955 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:270)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:147)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2018-11-30 10:54:26,955 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:270)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:147)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "HADOOP_EXAMPLES_JAR=\"${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar\"\n",
    "\n",
    "cd ./data\n",
    "\n",
    "# Split file\n",
    "split --additional-suffix=\".txt\" biblia.txt\n",
    "\n",
    "mkdir -p ./archivos_biblia\n",
    "mv xa*.txt ./archivos_biblia\n",
    "\n",
    "# Upload files\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p biblia/input\n",
    "$HADOOP_HOME/bin/hdfs dfs -put -f ./archivos_biblia/* biblia/input\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -ls biblia/input\n",
    "\n",
    "# Execute wordcount\n",
    "$HADOOP_HOME/bin/hdfs dfs -rm -r biblia/output\n",
    "$HADOOP_HOME/bin/hadoop --loglevel WARN jar $HADOOP_EXAMPLES_JAR wordcount biblia/input biblia/output\n",
    "\n",
    "# Show results\n",
    "$HADOOP_HOME/bin/hdfs dfs -cat biblia/output/part-r-00000 | grep --contex=5 \"^él\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otros ejercicios\n",
    "\n",
    "La implementación de los siguientes ejercicios se encuentra en (https://github.com/jsnavarroa/hadoop3-java-example). A continuación se muestran solo los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calcular el tf*idf de cada término de la colección anterior (Biblia) usando mapreduce (se debe analizar y modificar  el archivo wordcount de java) . Donde tf es la frecuencia del término e idf es la frecuencia inversa del término en la colección de documentos. Se puede calcular como:\n",
    "\n",
    "idf (t) =  log (|D| /(1+ numero de documentos donde aparece t)\n",
    "\n",
    "donde D es el número de documentos en la colección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Número de palabras/términos que comiencen con cada una de las vocales \n",
    "\n",
    "El mapper ignora los acentos y la capitalización. Por ejemplo, las palabras que comienzan con 'a', 'á', o 'A', se cuentan para 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t13108\n",
      "e\t19385\n",
      "i\t1331\n",
      "o\t3276\n",
      "u\t1687\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -cat biblia/p2/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Palabra más frecuente por cada una de las letras del abecedario\n",
    "\n",
    "El mapper no distingue la diferencia entre mayúsculas y minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\tDios?\n",
      "a\tcuales\n",
      "b\trecibirán\n",
      "c\tcomienza\n",
      "d\tde\n",
      "e\tdesconocida.\n",
      "f\tfue\n",
      "g\tángel\n",
      "h\tmuchos\n",
      "i\thiguera\n",
      "j\tHijo\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -cat biblia/p3/part-r-00000 | grep --contex=5 \"^e\"\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -get -f biblia/p3/part-r-00000 ./data/taller6_out/otros_p3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Los documentos donde la palabra más frecuente se encuentre.\n",
    "\n",
    "## 5. Calcular la co-ocurrencia de todos los pares de términos en la colección de documentos. La salida del reducer debe ser: la clave (pareja de términos) y valor (la frecuencia en la que aparecen los dos términos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementar un contador de kmer sobre un genoma usando hadoop. (Similar a word count, pero para identificar los kmers se realizan corrimientos sobre la cadena de ADN de acuerdo al tamaño) . La función debe recibir la cadena y el tamaño k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "\n",
    "# Uncompress data\n",
    "gzip -dkf ./data/ecoli.fa.gz\n",
    "\n",
    "# Remove first line of file\n",
    "tail -n +2 ./data/ecoli.fa > ./data/ecoli.txt\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p ecoli/input\n",
    "\n",
    "# Upload data\n",
    "$HADOOP_HOME/bin/hdfs dfs -put -f ./data/ecoli.txt ecoli/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAA\t104317\n",
      "AAC\t79028\n",
      "AAG\t60678\n",
      "AAT\t79476\n",
      "ACA\t56161\n",
      "ACC\t71689\n",
      "ACG\t70094\n",
      "ACT\t47752\n",
      "AGA\t54161\n",
      "AGC\t77456\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "K=3\n",
    "OUTPUT_FILE=\"./data/taller6_out/ecoli-${K}-mers.txt\"\n",
    "\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -cat ecoli/${K}-kmer/part-r-00000 > $OUTPUT_FILE\n",
    "\n",
    "cat $OUTPUT_FILE | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Usando el genoma de la ecoli que se encuentra en: ecoli.fa.gz\n",
    "\n",
    "### 7.1  ¿Cuáles son los 10 más frecuentes 5-mers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12410\tCCAGC\n",
      "12382\tCGCCA\n",
      "12033\tGCCAG\n",
      "11986\tGCTGG\n",
      "11971\tCTGGC\n",
      "11838\tTGGCG\n",
      "11326\tCAGCG\n",
      "11322\tCAGCA\n",
      "11111\tCGCTG\n",
      "10795\tTTTTT\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "K=5\n",
    "OUTPUT_FILE=\"./data/taller6_out/ecoli-${K}-mers.txt\"\n",
    "\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -cat ecoli/${K}-kmer-sort/part-r-00000 | sort -nbr > $OUTPUT_FILE\n",
    "\n",
    "cat $OUTPUT_FILE | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2  ¿Cuáles son los 10 más frecuentes 9-mers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\tCCAGCGCCA\n",
      "247\tCAGCGCCAG\n",
      "234\tGCGCTGGCG\n",
      "220\tCGCCAGCAG\n",
      "219\tCGCTGGCGG\n",
      "212\tCTGGCGCTG\n",
      "211\tCGCCAGCGC\n",
      "207\tGCCAGCGCC\n",
      "200\tTGGCGCTGG\n",
      "199\tCCGCCAGCA\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "K=9\n",
    "OUTPUT_FILE=\"./data/taller6_out/ecoli-${K}-mers.txt\"\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -cat ecoli/${K}-kmer-sort/part-r-00000 | sort -nbr > $OUTPUT_FILE\n",
    "\n",
    "cat $OUTPUT_FILE | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
