{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 6 - Hadoop - HDFS\n",
    "\n",
    "Juan Navarro, <jsnavarroa@unal.edu.co>\n",
    "\n",
    "\n",
    "# Instalación\n",
    "\n",
    "```bash\n",
    "cd \"${HOME}/worskpace/BDA\"\n",
    "\n",
    "git clone https://github.com/jsnavarroa/docker-hadoop.git\n",
    "cd docker-hadoop\n",
    "\n",
    "# Install docker-compose\n",
    "conda create --name py3 python=3.6\n",
    "conda activate py3\n",
    "conda install -c conda-forge docker-compose\n",
    "\n",
    "# Run local\n",
    "docker-compose -f docker-compose-local.yml build\n",
    "docker-compose -f docker-compose-local.yml up -d\n",
    "\n",
    "```\n",
    "\n",
    "* Hadoop URLs:\n",
    "  * NameNode http://localhost:9870/dfshealth.html#tab-overview.\n",
    "  * HDFS hdfs://localhost:9800.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Enviroment variables\n",
    "#export JAVA_HOME=$(readlink -f /usr/bin/java | sed \"s:bin/java::\")\n",
    "\n",
    "echo \"JAVA_HOME=$JAVA_HOME\"\n",
    "\n",
    "mkdir -p \"./data/taller6_out/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica HDFS\n",
    "\n",
    "Después de instalado el cluster de Hadoop se ejecutan los siguientes comandos para la práctica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /casa\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-25 20:22 /casa/piso1\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-25 20:22 /casa/piso1/cocina\n",
      "-rw-r--r--   3 juan supergroup       5194 2018-11-25 20:22 /casa/piso1/cocina/estufa.jpg\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-25 20:22 /casa/piso1/sala\n",
      "-rw-r--r--   3 juan supergroup       3106 2018-11-25 20:22 /casa/piso1/sala/mesa.jpg\n",
      "-rw-r--r--   3 juan supergroup       5688 2018-11-25 20:22 /casa/piso1/sala/televisor.jpg\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-25 20:22 /casa/piso2\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-25 20:22 /casa/piso2/alcoba\n",
      "-rw-r--r--   3 juan supergroup       5859 2018-11-25 20:22 /casa/piso2/alcoba/cama.jpg\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-25 20:22 /casa/piso2/estudio\n",
      "-rw-r--r--   3 juan supergroup       9216 2018-11-25 20:22 /casa/piso2/estudio/libro.doc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "\n",
    "# Cleaning\n",
    "$HADOOP_HOME/bin/hdfs dfs -rm -r /casa\n",
    "\n",
    "# Create house\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p /casa/piso1/sala\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p /casa/piso1/cocina\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p /casa/piso2/alcoba\n",
    "\n",
    "# Add furniture\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/mesa.jpg /casa/piso1/sala\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/estufa.jpg /casa/piso1/cocina\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/libro.doc /casa/piso2/alcoba\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/televisor.jpg /casa/piso2/alcoba\n",
    "\n",
    "# Renovation\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p /casa/piso2/estudio\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/cama.jpg /casa/piso2/alcoba\n",
    "$HADOOP_HOME/bin/hdfs dfs -mv /casa/piso2/alcoba/televisor.jpg /casa/piso1/sala/televisor.jpg\n",
    "$HADOOP_HOME/bin/hdfs dfs -mv /casa/piso2/alcoba/libro.doc /casa/piso2/estudio/libro.doc\n",
    "\n",
    "# Check\n",
    "$HADOOP_HOME/bin/hdfs dfs -ls -R /casa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio de conteo de términos\n",
    "\n",
    "## I. Computar el valor de pi en paralelo en 5 nodos con 5 \"samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Maps  = 5\n",
      "Samples per Map = 5\n",
      "Wrote input for Map #0\n",
      "Wrote input for Map #1\n",
      "Wrote input for Map #2\n",
      "Wrote input for Map #3\n",
      "Wrote input for Map #4\n",
      "Starting Job\n",
      "2018-11-25 20:22:24,865 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-11-25 20:22:24,898 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:270)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:147)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2018-11-25 20:22:24,903 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:270)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:147)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Job Finished in 1.38 seconds\n",
      "Estimated value of Pi is 3.68000000000000000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "HADOOP_EXAMPLES_JAR=\"${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar\"\n",
    "\n",
    "$HADOOP_HOME/bin/hadoop --loglevel WARN jar $HADOOP_EXAMPLES_JAR pi 5 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Frecuencia de Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\n",
      "-rw-r--r--   3 juan supergroup     111448 2018-11-28 22:36 biblia/input/xaa.txt\n",
      "-rw-r--r--   3 juan supergroup     127578 2018-11-28 22:36 biblia/input/xab.txt\n",
      "-rw-r--r--   3 juan supergroup     151478 2018-11-28 22:36 biblia/input/xac.txt\n",
      "-rw-r--r--   3 juan supergroup     190377 2018-11-28 22:36 biblia/input/xad.txt\n",
      "-rw-r--r--   3 juan supergroup     167416 2018-11-28 22:36 biblia/input/xae.txt\n",
      "-rw-r--r--   3 juan supergroup     131772 2018-11-28 22:36 biblia/input/xaf.txt\n",
      "-rw-r--r--   3 juan supergroup     121707 2018-11-28 22:36 biblia/input/xag.txt\n",
      "Deleted biblia/output\n",
      "áspides\t1\n",
      "átate\t1\n",
      "échala\t1\n",
      "échalo\t3\n",
      "échate\t4\n",
      "él\t592\n",
      "él!\t1\n",
      "él,\t215\n",
      "él.\t131\n",
      "él:\t8\n",
      "él;\t43\n",
      "él?\t8\n",
      "éramos\t5\n",
      "ése\t16\n",
      "ése,\t1\n",
      "ésta\t6\n",
      "ésta,\t4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-28 22:36:40,094 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "HADOOP_EXAMPLES_JAR=\"${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar\"\n",
    "\n",
    "cd ./data\n",
    "\n",
    "# Split file\n",
    "split --additional-suffix=\".txt\" biblia.txt\n",
    "\n",
    "mkdir -p ./archivos_biblia\n",
    "mv xa*.txt ./archivos_biblia\n",
    "\n",
    "# Upload files\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p biblia/input\n",
    "$HADOOP_HOME/bin/hdfs dfs -put -f ./archivos_biblia/* biblia/input\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -ls biblia/input\n",
    "\n",
    "# Execute wordcount\n",
    "$HADOOP_HOME/bin/hdfs dfs -rm -r biblia/output\n",
    "$HADOOP_HOME/bin/hadoop --loglevel WARN jar $HADOOP_EXAMPLES_JAR wordcount biblia/input biblia/output\n",
    "\n",
    "# Show results\n",
    "$HADOOP_HOME/bin/hdfs dfs -cat biblia/output/part-r-00000 | grep --contex=5 \"^él\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otros ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calcular el tf*idf de cada término de la colección anterior (Biblia) usando mapreduce (se debe analizar y modificar  el archivo wordcount de java) . Donde tf es la frecuencia del término e idf es la frecuencia inversa del término en la colección de documentos. Se puede calcular como:\n",
    "\n",
    "idf (t) =  log (|D| /(1+ numero de documentos donde aparece t)\n",
    "\n",
    "donde D es el número de documentos en la colección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.lang.ClassCastException: org.apache.hadoop.hdfs.DistributedFileSystem cannot be cast to org.apache.hadoop.fs.FileSystem\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3353)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3403)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3371)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:477)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:226)\n",
      "\tat co.edu.unal.bda.hadoop.WordCount.main(WordCount.java:82)\n",
      "\tat co.edu.unal.bda.hadoop.BeakerWrapperClass1261714175Idf294ca399bcd420085e0e58769cfd929.beakerRun(BeakerWrapperClass1261714175Idf294ca399bcd420085e0e58769cfd929.java:35)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat com.twosigma.beakerx.javash.evaluator.JavaCodeRunner.compileAndRunCode(JavaCodeRunner.java:121)\n",
      "\tat com.twosigma.beakerx.javash.evaluator.JavaCodeRunner.compileCode(JavaCodeRunner.java:99)\n",
      "\tat com.twosigma.beakerx.javash.evaluator.JavaCodeRunner.runCode(JavaCodeRunner.java:84)\n",
      "\tat com.twosigma.beakerx.javash.evaluator.JavaCodeRunner.call(JavaCodeRunner.java:58)\n",
      "\tat com.twosigma.beakerx.javash.evaluator.JavaCodeRunner.call(JavaCodeRunner.java:39)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package co.edu.unal.bda.hadoop;\n",
    "\n",
    "try {\n",
    "\tWordCount.main(\"biblia/input\",\"biblia/p1\");\n",
    "} catch (Exception e) {\n",
    "    e.printStackTrace();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Número de palabras/términos que comiencen con cada una de las vocales \n",
    "\n",
    "El mapper ignora los acentos y la capitalización. Por ejemplo, las palabras que comienzan con 'a', 'á', o 'A', se cuentan para 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\t139893\n",
      "a\t13108\n",
      "e\t19385\n",
      "i\t1331\n",
      "o\t3276\n",
      "u\t1687\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -cat biblia/p2/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Palabra más frecuente por cada una de las letras del abecedario\n",
    "\n",
    "El mapper no distingue la diferencia entre mayúsculas y minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\tDios?\n",
      "a\tcuales\n",
      "b\trecibirán\n",
      "c\tcomienza\n",
      "d\tde\n",
      "e\tdesconocida.\n",
      "f\tfue\n",
      "g\tángel\n",
      "h\tmuchos\n",
      "i\thiguera\n",
      "j\tHijo\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -cat biblia/p3/part-r-00000 | grep --contex=5 \"^e\"\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -get -f biblia/p3/part-r-00000 ./data/taller6_out/otros_p3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementar un contador de kmer sobre un genoma usando hadoop. (Similar a word count, pero para identificar los kmers se realizan corrimientos sobre la cadena de ADN de acuerdo al tamaño) . La función debe recibir la cadena y el tamaño k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./data/ecoli.fa.gz:\t 69.6% -- replaced with ./data/ecoli.fa\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "\n",
    "# Uncompress data\n",
    "gzip -dkf ./data/ecoli.fa.gz\n",
    "\n",
    "# Remove first line of file\n",
    "tail -n +2 ./data/ecoli.fa > ./data/ecoli.txt\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p ecoli/input\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -put -f ./data/ecoli.txt ecoli/input\n",
    "\n",
    "#$HADOOP_HOME/bin/hdfs dfs -cat biblia/p6/part-r-00000 | grep --contex=5 \"^e\"\n",
    "\n",
    "#$HADOOP_HOME/bin/hdfs dfs -get -f biblia/p6/part-r-00000 ./data/taller6_out/otros_p6.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
