{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 6 - Hadoop - HDFS\n",
    "\n",
    "Juan Navarro, <jsnavarroa@unal.edu.co>\n",
    "\n",
    "\n",
    "# Instalación\n",
    "\n",
    "```bash\n",
    "cd \"${HOME}/worskpace/BDA\"\n",
    "\n",
    "git clone https://github.com/jsnavarroa/docker-hadoop.git\n",
    "cd docker-hadoop\n",
    "\n",
    "# Install docker-compose\n",
    "conda create --name py3 python=3.6\n",
    "conda activate py3\n",
    "conda install -c conda-forge docker-compose\n",
    "\n",
    "# Run local\n",
    "docker-compose -f docker-compose-local.yml build\n",
    "docker-compose -f docker-compose-local.yml up -d\n",
    "\n",
    "```\n",
    "\n",
    "* Hadoop URLs:\n",
    "  * NameNode http://localhost:9870/dfshealth.html#tab-overview.\n",
    "  * HDFS hdfs://localhost:9800.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Enviroment variables\n",
    "#export JAVA_HOME=$(readlink -f /usr/bin/java | sed \"s:bin/java::\")\n",
    "\n",
    "echo \"JAVA_HOME=$JAVA_HOME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica HDFS\n",
    "\n",
    "Después de instalado el cluster de Hadoop se ejecutan los siguientes comandos para la práctica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /casa\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-25 20:22 /casa/piso1\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-25 20:22 /casa/piso1/cocina\n",
      "-rw-r--r--   3 juan supergroup       5194 2018-11-25 20:22 /casa/piso1/cocina/estufa.jpg\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-25 20:22 /casa/piso1/sala\n",
      "-rw-r--r--   3 juan supergroup       3106 2018-11-25 20:22 /casa/piso1/sala/mesa.jpg\n",
      "-rw-r--r--   3 juan supergroup       5688 2018-11-25 20:22 /casa/piso1/sala/televisor.jpg\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-25 20:22 /casa/piso2\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-25 20:22 /casa/piso2/alcoba\n",
      "-rw-r--r--   3 juan supergroup       5859 2018-11-25 20:22 /casa/piso2/alcoba/cama.jpg\n",
      "drwxr-xr-x   - juan supergroup          0 2018-11-25 20:22 /casa/piso2/estudio\n",
      "-rw-r--r--   3 juan supergroup       9216 2018-11-25 20:22 /casa/piso2/estudio/libro.doc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "\n",
    "# Cleaning\n",
    "$HADOOP_HOME/bin/hdfs dfs -rm -r /casa\n",
    "\n",
    "# Create house\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p /casa/piso1/sala\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p /casa/piso1/cocina\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p /casa/piso2/alcoba\n",
    "\n",
    "# Add furniture\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/mesa.jpg /casa/piso1/sala\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/estufa.jpg /casa/piso1/cocina\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/libro.doc /casa/piso2/alcoba\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/televisor.jpg /casa/piso2/alcoba\n",
    "\n",
    "# Renovation\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p /casa/piso2/estudio\n",
    "$HADOOP_HOME/bin/hdfs dfs -put ./resources/cama.jpg /casa/piso2/alcoba\n",
    "$HADOOP_HOME/bin/hdfs dfs -mv /casa/piso2/alcoba/televisor.jpg /casa/piso1/sala/televisor.jpg\n",
    "$HADOOP_HOME/bin/hdfs dfs -mv /casa/piso2/alcoba/libro.doc /casa/piso2/estudio/libro.doc\n",
    "\n",
    "# Check\n",
    "$HADOOP_HOME/bin/hdfs dfs -ls -R /casa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio de conteo de términos\n",
    "\n",
    "## I. Computar el valor de pi en paralelo en 5 nodos con 5 \"samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Maps  = 5\n",
      "Samples per Map = 5\n",
      "Wrote input for Map #0\n",
      "Wrote input for Map #1\n",
      "Wrote input for Map #2\n",
      "Wrote input for Map #3\n",
      "Wrote input for Map #4\n",
      "Starting Job\n",
      "2018-11-25 20:22:24,865 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-11-25 20:22:24,898 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:270)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:147)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2018-11-25 20:22:24,903 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:270)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:147)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Job Finished in 1.38 seconds\n",
      "Estimated value of Pi is 3.68000000000000000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "HADOOP_EXAMPLES_JAR=\"${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar\"\n",
    "\n",
    "$HADOOP_HOME/bin/hadoop --loglevel WARN jar $HADOOP_EXAMPLES_JAR pi 5 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Frecuencia de Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\n",
      "-rw-r--r--   3 juan supergroup     111448 2018-11-25 20:22 biblia/input/xaa.txt\n",
      "-rw-r--r--   3 juan supergroup     127578 2018-11-25 20:22 biblia/input/xab.txt\n",
      "-rw-r--r--   3 juan supergroup     151478 2018-11-25 20:22 biblia/input/xac.txt\n",
      "-rw-r--r--   3 juan supergroup     190377 2018-11-25 20:22 biblia/input/xad.txt\n",
      "-rw-r--r--   3 juan supergroup     167416 2018-11-25 20:22 biblia/input/xae.txt\n",
      "-rw-r--r--   3 juan supergroup     131772 2018-11-25 20:22 biblia/input/xaf.txt\n",
      "-rw-r--r--   3 juan supergroup     121707 2018-11-25 20:22 biblia/input/xag.txt\n",
      "Deleted biblia/output\n",
      "2018-11-25 20:22:35,050 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "árboles\t2\n",
      "árboles,\t4\n",
      "árboles.\t1\n",
      "árboles;\t2\n",
      "ásperos\t2\n",
      "áspides\t1\n",
      "átate\t1\n",
      "échala\t1\n",
      "échalo\t3\n",
      "échate\t4\n",
      "él\t592\n",
      "él!\t1\n",
      "él,\t215\n",
      "él.\t131\n",
      "él:\t8\n",
      "él;\t43\n",
      "él?\t8\n",
      "éramos\t5\n",
      "ése\t16\n",
      "ése,\t1\n",
      "ésta\t6\n",
      "ésta,\t4\n",
      "éstas\t3\n",
      "éste\t107\n",
      "éste,\t25\n",
      "éste.\t1\n",
      "éste:\t3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "HADOOP_EXAMPLES_JAR=\"${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar\"\n",
    "\n",
    "cd ./data\n",
    "\n",
    "# Split file\n",
    "split --additional-suffix=\".txt\" biblia.txt\n",
    "\n",
    "mkdir -p ./archivos_biblia\n",
    "mv xa*.txt ./archivos_biblia\n",
    "\n",
    "# Upload files\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p biblia/input\n",
    "$HADOOP_HOME/bin/hdfs dfs -put -f ./archivos_biblia/* biblia/input\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -ls biblia/input\n",
    "\n",
    "# Execute wordcount\n",
    "$HADOOP_HOME/bin/hdfs dfs -rm -r biblia/output\n",
    "$HADOOP_HOME/bin/hadoop --loglevel WARN jar $HADOOP_EXAMPLES_JAR wordcount biblia/input biblia/output\n",
    "\n",
    "# Show results\n",
    "$HADOOP_HOME/bin/hdfs dfs -cat biblia/output/part-r-00000 | grep --contex=10 \"^él\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otros ejercicios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available magic commands:\n",
      "%%javascript \n",
      "%%js \n",
      "%%html \n",
      "%%HTML \n",
      "%%bash \n",
      "%lsmagic \n",
      "%classpath add jar <jar path>\n",
      "%classpath add mvn <group name version>\n",
      "%%classpath add mvn <group name version>\n",
      "%classpath add dynamic \n",
      "%classpath config resolver <repoName repoUrl>\n",
      "%classpath reset \n",
      "%classpath \n",
      "%import static <classpath>\n",
      "%import <classpath>\n",
      "%unimport <classpath>\n",
      "%time \n",
      "%%time \n",
      "%timeit \n",
      "%%timeit \n",
      "%load_magic \n",
      "%%kernel \n",
      "%%python \n",
      "%%clojure \n",
      "%%groovy \n",
      "%%java \n",
      "%%kotlin \n",
      "%%scala \n",
      "%%sql \n",
      "%%async \n"
     ]
    }
   ],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add mvn org.apache.hadoop hadoop-client 3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "co.edu.unal.bda.hadoop.WordCount"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package co.edu.unal.bda.hadoop;\n",
    "\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.FileSystem;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class WordCount {\n",
    "\n",
    "\tprivate static final Logger log = LoggerFactory.getLogger(WordCount.class);\n",
    "\n",
    "\tpublic static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {\n",
    "\n",
    "\t\tprivate final static IntWritable one = new IntWritable(1);\n",
    "\t\tprivate Text word = new Text();\n",
    "\n",
    "\t\tpublic void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n",
    "\t\t\tStringTokenizer itr = new StringTokenizer(value.toString());\n",
    "\t\t\twhile (itr.hasMoreTokens()) {\n",
    "\t\t\t\tword.set(itr.nextToken());\n",
    "\t\t\t\tcontext.write(word, one);\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\tpublic static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "\t\tprivate IntWritable result = new IntWritable();\n",
    "\n",
    "\t\tpublic void reduce(Text key, Iterable<IntWritable> values, Context context)\n",
    "\t\t\t\tthrows IOException, InterruptedException {\n",
    "\t\t\tint sum = 0;\n",
    "\t\t\tfor (IntWritable val : values) {\n",
    "\t\t\t\tsum += val.get();\n",
    "\t\t\t}\n",
    "\t\t\tresult.set(sum);\n",
    "\t\t\tcontext.write(key, result);\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\tpublic static void main(String input, String output) throws Exception {\n",
    "\t\tConfiguration conf = new Configuration();\n",
    "\t\tconf.set(\"fs.defaultFS\", \"hdfs://localhost:9800\");\n",
    "\t\tconf.set(\"fs.hdfs.impl\", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());\n",
    "\n",
    "\t\tJob job = Job.getInstance(conf, \"word count\");\n",
    "\t\tjob.setJarByClass(WordCount.class);\n",
    "\t\tjob.setMapperClass(TokenizerMapper.class);\n",
    "\t\tjob.setCombinerClass(IntSumReducer.class);\n",
    "\t\tjob.setReducerClass(IntSumReducer.class);\n",
    "\t\tjob.setOutputKeyClass(Text.class);\n",
    "\t\tjob.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "\t\t// Overwrite output\n",
    "\t\tFileSystem fileSystem = FileSystem.get(conf);\n",
    "\t\tPath outputPath = new Path(output);\n",
    "\t\tif (fileSystem.exists(outputPath)) {\n",
    "\t\t\tfileSystem.delete(outputPath, true);\n",
    "\t\t}\n",
    "\t\tfileSystem.close();\n",
    "\n",
    "\t\tFileInputFormat.addInputPath(job, new Path(input));\n",
    "\n",
    "\t\tFileOutputFormat.setOutputPath(job, outputPath);\n",
    "\n",
    "\t\tjob.waitForCompletion(true);\n",
    "\n",
    "\t\tlog.info(\"Work done\");\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calcular el tf*idf de cada término de la colección anterior (Biblia) usando mapreduce (se debe analizar y modificar  el archivo wordcount de java) . Donde tf es la frecuencia del término e idf es la frecuencia inversa del término en la colección de documentos. Se puede calcular como:\n",
    "\n",
    "idf (t) =  log (|D| /(1+ numero de documentos donde aparece t)\n",
    "\n",
    "donde D es el número de documentos en la colección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.lang.ClassCastException: org.apache.hadoop.hdfs.DistributedFileSystem cannot be cast to org.apache.hadoop.fs.FileSystem\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3353)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3403)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3371)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:477)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:226)\n",
      "\tat co.edu.unal.bda.hadoop.WordCount.main(WordCount.java:82)\n",
      "\tat co.edu.unal.bda.hadoop.BeakerWrapperClass1261714175Idf294ca399bcd420085e0e58769cfd929.beakerRun(BeakerWrapperClass1261714175Idf294ca399bcd420085e0e58769cfd929.java:35)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat com.twosigma.beakerx.javash.evaluator.JavaCodeRunner.compileAndRunCode(JavaCodeRunner.java:121)\n",
      "\tat com.twosigma.beakerx.javash.evaluator.JavaCodeRunner.compileCode(JavaCodeRunner.java:99)\n",
      "\tat com.twosigma.beakerx.javash.evaluator.JavaCodeRunner.runCode(JavaCodeRunner.java:84)\n",
      "\tat com.twosigma.beakerx.javash.evaluator.JavaCodeRunner.call(JavaCodeRunner.java:58)\n",
      "\tat com.twosigma.beakerx.javash.evaluator.JavaCodeRunner.call(JavaCodeRunner.java:39)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package co.edu.unal.bda.hadoop;\n",
    "\n",
    "try {\n",
    "\tWordCount.main(\"biblia/input\",\"biblia/p1\");\n",
    "} catch (Exception e) {\n",
    "    e.printStackTrace();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2\t1\n",
      "(Como\t1\n",
      "(Hablo\t1\n",
      "(Hch.\t3\n",
      "(Jn.\t1\n",
      "(Judas\t1\n",
      "(Lc.\t35\n",
      "(María,\t1\n",
      "(Mr.\t84\n",
      "(Mt.\t188\n",
      "(Porque\t2\n",
      "(aunque\t2\n",
      "(como\t5\n",
      "(dice\t1\n",
      "(digo\t1\n",
      "cat: Unable to write to output stream.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOP_HOME=\"${HOME}/Programas/BDA/hadoop-3.1.1\"\n",
    "\n",
    "$HADOOP_HOME/bin/hdfs dfs -cat biblia/p1/part-r-00000 | head -n 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "text/x-java",
   "file_extension": ".java",
   "mimetype": "",
   "name": "Java",
   "nbconverter_exporter": "",
   "version": "1.8.0_152-release"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
